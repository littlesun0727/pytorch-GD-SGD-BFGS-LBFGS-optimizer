{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    " \n",
    "dataFile = './data1.mat'\n",
    "data = scio.loadmat(dataFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'TrainingX', 'TrainingY', 'TestX', 'TestY'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = data[\"TrainingX\"], data[\"TrainingY\"], data[\"TestX\"], data[\"TestY\"]\n",
    "# x_train, y_train, x_test, y_test = np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X,Y):\n",
    "    N1, N2 = len(X), len(Y)\n",
    "    X_norm = np.sum(X ** 2, axis = -1)\n",
    "    Y_norm = np.sum(Y ** 2, axis = -1)\n",
    "    K = X_norm[:,None] + Y_norm[None,:] - 2 * np.dot(X, Y.T)\n",
    "    sigma_2 = np.sum(K)/(N2**2)\n",
    "    K = np.exp(-K/(2*sigma_2))\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kx_train, kx_test = kernel(x_train,x_train), kernel(x_test, x_train)\n",
    "\n",
    "dtype = torch.float32 \n",
    "# put tensor on cpu(or you can try GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "KX_train = torch.tensor(kx_train, dtype=dtype, device=device)\n",
    "Y_train = torch.tensor(y_train)\n",
    "\n",
    "KX_test = torch.tensor(kx_test, dtype=dtype, device=device)\n",
    "Y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_accu(KX_test, Y_test, w, lambda_param):\n",
    "    Z = torch.mm(KX_test, w)\n",
    "    p = 1.0/(1.0+torch.exp(-Z))\n",
    "    pred = (p>0.5)*2-1\n",
    "    correct = pred.eq(Y_test.view_as(pred)).sum().item()\n",
    "    accu = correct/len(Y_test)\n",
    "\n",
    "    A = 1.0/(1.0+torch.exp(-Z*Y_test))\n",
    "    l2_regularization = torch.sum(w ** 2)\n",
    "    loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "    return loss, accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(KX_train, Y_train, KX_test, Y_test, n_iterations, learning_rate, lambda_param, epsilon):   \n",
    "    \n",
    "    N = 10000\n",
    "    w = torch.zeros((N,1), device=device, requires_grad = True)\n",
    "    torch.nn.init.kaiming_uniform_(w, a=math.sqrt(1000))\n",
    "\n",
    "    \n",
    "    try:\n",
    "        filename = './results/GD_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "        os.remove(filename)\n",
    "        print('Removed previous results!')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    t0 = time.time()\n",
    "    t_test = 0\n",
    "    for i in range(1, n_iterations + 1):\n",
    "\n",
    "        Z = torch.mm(KX_train, w)\n",
    "        p = 1.0/(1.0+torch.exp(-Z))\n",
    "        pred = (p>0.5)*2-1\n",
    "        correct = pred.eq(Y_train.view_as(pred)).sum().item()\n",
    "        accu = correct/len(Y_train)\n",
    "\n",
    "        A = 1.0/(1.0+torch.exp(-Z*Y_train))\n",
    "        l2_regularization = torch.sum(w ** 2)\n",
    "        loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "        if w.grad is not None:\n",
    "            w.grad.zero_()  # 1\n",
    "\n",
    "        loss.backward()  # 2\n",
    "\n",
    "        with torch.no_grad():  # 3\n",
    "            w -= learning_rate * w.grad\n",
    "\n",
    "\n",
    "        if i == 1 or i % 25 == 0:\n",
    "            t_test_0 = time.time()\n",
    "            test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "            t_test += time.time()-t_test_0\n",
    "            \n",
    "            result = \"iteration {}: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(i, time.time()-t_test-t0,loss, accu, test_loss, test_accu)\n",
    "#             print(result)\n",
    "            filename = './results/GD_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "            with open(filename, 'a') as fp: \n",
    "                fp.write(result+'\\n')\n",
    "\n",
    "        if w.grad.norm()<epsilon:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed previous results!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-de2d82e07550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-63243b67cd94>\u001b[0m in \u001b[0;36mGD\u001b[0;34m(KX_train, Y_train, KX_test, Y_test, n_iterations, learning_rate, lambda_param, epsilon)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iterations = 10000 \n",
    "learning_rates = [0.005]\n",
    "lambda_param = 1e-4\n",
    "epsilon = 1e-5\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    GD(KX_train, Y_train, KX_test, Y_test, n_iterations, learning_rate, lambda_param, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(KX_train, Y_train, KX_test, Y_test, n_epoches,batch_size, learning_rate, lambda_param, epsilon):\n",
    "    N = 10000\n",
    "    w = torch.zeros((N,1), device=device, requires_grad = True)\n",
    "    torch.nn.init.kaiming_uniform_(w, a=math.sqrt(1000))\n",
    "    \n",
    "    try:\n",
    "        filename = './results/SGD_{}_{}_{}.txt'.format(learning_rate, lambda_param,batch_size)\n",
    "        os.remove(filename)\n",
    "        print('Removed previous results!')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_dataset = TensorDataset(KX_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "        batch_size=batch_size, shuffle=True, sampler=None,\n",
    "        num_workers=4, pin_memory=True)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    t_test = 0\n",
    "    for epoch in range(1, n_epoches + 1):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            Z = torch.mm(data, w)\n",
    "            p = 1.0/(1.0+torch.exp(-Z))\n",
    "            pred = (p>0.5)*2-1\n",
    "            correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "            accu = correct/len(target)\n",
    "\n",
    "            A = 1.0/(1.0+torch.exp(-Z*target))\n",
    "            l2_regularization = torch.sum(w ** 2)\n",
    "            loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "            if w.grad is not None:\n",
    "                w.grad.zero_()  # 1\n",
    "\n",
    "            loss.backward()  # 2\n",
    "\n",
    "            with torch.no_grad():  # 3\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "\n",
    "            if (batch_idx*batch_size) % 500 == 0:\n",
    "                t_test_0 = time.time()\n",
    "                test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "                t_test += time.time()-t_test_0\n",
    "                \n",
    "                result = \"epoch {}[{}/{} ({:.0f}%)]: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(epoch, batch_idx*batch_size, len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), time.time()-t0, loss, accu, test_loss, test_accu)\n",
    "#                 print(result)\n",
    "                filename = './results/SGD_{}_{}_{}.txt'.format(learning_rate, lambda_param,batch_size)\n",
    "                with open(filename, 'a') as fp: \n",
    "                    fp.write(result+'\\n')\n",
    "\n",
    "            if w.grad.norm()<epsilon:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoches = 100\n",
    "batch_sizes = [1, 10, 100]\n",
    "learning_rates =[0.001, 0.005]\n",
    "lambda_param =1e-4\n",
    "epsilon = 1e-5\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        SGD(KX_train, Y_train, KX_test, Y_test, n_epoches, batch_size, learning_rate, lambda_param, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = np.vstack((x_train[:2000],x_train[-2000:]))\n",
    "y_train2 = np.vstack((y_train[:2000],y_train[-2000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kx_train2, kx_test2 = kernel(x_train2,x_train2), kernel(x_test, x_train2)\n",
    "\n",
    "dtype = torch.float32 \n",
    "# put tensor on cpu(or you can try GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "KX_train2 = torch.tensor(kx_train2, dtype=dtype, device=device)\n",
    "Y_train2 = torch.tensor(y_train2)\n",
    "\n",
    "KX_test2 = torch.tensor(kx_test2, dtype=dtype, device=device)\n",
    "Y_test2 = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BFGS(KX_train, Y_train, KX_test, Y_test, n_iterations, learning_rate, lambda_param, epsilon):\n",
    "    N = len(KX_train)\n",
    "    w = torch.zeros((N,1), device=device, requires_grad = True)\n",
    "    torch.nn.init.kaiming_uniform_(w, a=math.sqrt(1000))\n",
    "    \n",
    "    try:\n",
    "        filename = './results/BFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "        os.remove(filename)\n",
    "        print('Removed previous results!')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_dataset = TensorDataset(KX_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "        batch_size=batch_size, shuffle=True, sampler=None,\n",
    "        num_workers=4, pin_memory=True)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    t_test = 0\n",
    "    \n",
    "    Z = torch.mm(KX_train, w)\n",
    "    p = 1.0/(1.0+torch.exp(-Z))\n",
    "    pred = (p>0.5)*2-1\n",
    "    correct = pred.eq(Y_train.view_as(pred)).sum().item()\n",
    "    accu = correct/len(Y_train)\n",
    "\n",
    "    A = 1.0/(1.0+torch.exp(-Z*Y_train))\n",
    "    l2_regularization = torch.sum(w ** 2)\n",
    "    loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()  # 1\n",
    "\n",
    "    loss.backward()  # 2\n",
    "    \n",
    "    t_test_0 = time.time()\n",
    "    test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "    t_test += time.time()-t_test_0\n",
    "    \n",
    "    result = \"iteration {}: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(1, time.time()-t_test-t0,loss, accu, test_loss, test_accu)\n",
    "    print(result)\n",
    "    filename = './results/BFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "    with open(filename, 'a') as fp: \n",
    "        fp.write(result+'\\n')\n",
    "\n",
    "    H, r, grad_past = torch.eye(N), torch.Tensor(N,1), torch.Tensor(N,1)\n",
    "    r.copy_(w.grad)\n",
    "    grad_past.copy_(w.grad)\n",
    "    \n",
    "    for i in range(2, n_iterations + 1):\n",
    "        \n",
    "        with torch.no_grad():  # 3\n",
    "            w -= learning_rate * r\n",
    "            \n",
    "        Z = torch.mm(KX_train, w)\n",
    "        p = 1.0/(1.0+torch.exp(-Z))\n",
    "        pred = (p>0.5)*2-1\n",
    "        correct = pred.eq(Y_train.view_as(pred)).sum().item()\n",
    "        accu = correct/len(Y_train)\n",
    "\n",
    "        A = 1.0/(1.0+torch.exp(-Z*Y_train))\n",
    "        l2_regularization = torch.sum(w ** 2)\n",
    "        loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "        if w.grad is not None:\n",
    "            w.grad.zero_()  # 1\n",
    "\n",
    "        loss.backward()  # 2\n",
    "        \n",
    "        s = -learning_rate * r\n",
    "        y = w.grad - grad_past\n",
    "        grad_past.copy_(w.grad)\n",
    "\n",
    "        p = 1.0/s.view(-1).dot(y.view(-1))\n",
    "        I = torch.eye(N)\n",
    "        H = torch.matmul(torch.matmul(I-p*torch.matmul(s,y.t()), H), I-p*torch.matmul(y,s.t()))+p*torch.matmul(s,s.t())\n",
    "\n",
    "        r = torch.matmul(H, w.grad)\n",
    "\n",
    "        if i == 1 or i % 1 == 0:\n",
    "            t_test_0 = time.time()\n",
    "            test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "            t_test += time.time()-t_test_0\n",
    "\n",
    "            result = \"iteration {}: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(i, time.time()-t_test-t0,loss, accu, test_loss, test_accu)\n",
    "#             print(result)\n",
    "            filename = './results/BFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "            with open(filename, 'a') as fp: \n",
    "                fp.write(result+'\\n')\n",
    "\n",
    "        if w.grad.norm()<epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "learning_rates = [0.005]\n",
    "lambda_param = 1e-4\n",
    "epsilon = 1e-5\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    BFGS(KX_train2, Y_train2, KX_test2, Y_test2, n_iterations, learning_rate, lambda_param, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBFGS(KX_train, Y_train, KX_test, Y_test, n_iterations, history_size, learning_rate, lambda_param, epsilon):\n",
    "    N = len(KX_train)\n",
    "    w = torch.zeros((N,1), device=device, requires_grad = True)\n",
    "    torch.nn.init.kaiming_uniform_(w, a=math.sqrt(1000))\n",
    "    \n",
    "    try:\n",
    "        filename = './results/LBFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "        os.remove(filename)\n",
    "        print('Removed previous results!')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_dataset = TensorDataset(KX_train, Y_train)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "        batch_size=batch_size, shuffle=True, sampler=None,\n",
    "        num_workers=4, pin_memory=True)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    t_test = 0\n",
    "    \n",
    "    Z = torch.mm(KX_train, w)\n",
    "    p = 1.0/(1.0+torch.exp(-Z))\n",
    "    pred = (p>0.5)*2-1\n",
    "    correct = pred.eq(Y_train.view_as(pred)).sum().item()\n",
    "    accu = correct/len(Y_train)\n",
    "\n",
    "    A = 1.0/(1.0+torch.exp(-Z*Y_train))\n",
    "    l2_regularization = torch.sum(w ** 2)\n",
    "    loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()  # 1\n",
    "\n",
    "    loss.backward()  # 2\n",
    "    \n",
    "    t_test_0 = time.time()\n",
    "    test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "    t_test += time.time()-t_test_0\n",
    "    \n",
    "    result = \"iteration {}: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(1, time.time()-t_test-t0,loss, accu, test_loss, test_accu)\n",
    "    print(result)\n",
    "    filename = './results/LBFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "    with open(filename, 'a') as fp: \n",
    "        fp.write(result+'\\n')\n",
    "\n",
    "    r, grad_past = torch.Tensor(N,1), torch.Tensor(N,1)\n",
    "    r.copy_(w.grad)\n",
    "    grad_past.copy_(w.grad)\n",
    "    \n",
    "    rhos, ss, ys = [], [], []\n",
    "    \n",
    "    for idx in range(2, n_iterations + 1):\n",
    "        \n",
    "        with torch.no_grad():  # 3\n",
    "            w -= learning_rate * r\n",
    "            \n",
    "        Z = torch.mm(KX_train, w)\n",
    "        p = 1.0/(1.0+torch.exp(-Z))\n",
    "        pred = (p>0.5)*2-1\n",
    "        correct = pred.eq(Y_train.view_as(pred)).sum().item()\n",
    "        accu = correct/len(Y_train)\n",
    "\n",
    "        A = 1.0/(1.0+torch.exp(-Z*Y_train))\n",
    "        l2_regularization = torch.sum(w ** 2)\n",
    "        loss = -torch.mean(torch.log(A.view(-1)))+lambda_param*l2_regularization\n",
    "        if w.grad is not None:\n",
    "            w.grad.zero_()  # 1\n",
    "\n",
    "        loss.backward()  # 2\n",
    "        \n",
    "        s = (-learning_rate * r).view(-1)\n",
    "        y = (w.grad - grad_past).view(-1)\n",
    "        grad_past.copy_(w.grad)\n",
    "        \n",
    "        q = torch.zeros(N)\n",
    "        q.copy_(w.grad.view(-1))\n",
    "        y_s = s.dot(y)\n",
    "        rho = 1.0/y_s\n",
    "        \n",
    "        \n",
    "        if len(ss)>=history_size:\n",
    "            ss.pop(0)\n",
    "            ys.pop(0)\n",
    "            rhos.pop(0)\n",
    "            \n",
    "        ss.append(s)\n",
    "        ys.append(y)\n",
    "        rhos.append(rho)\n",
    "        \n",
    "        H_diag = y_s/y.dot(y)\n",
    "        \n",
    "        num_old = len(ss)\n",
    "        al = [None] * history_size\n",
    "        for i in range(num_old - 1, -1, -1):\n",
    "            al[i] = ss[i].dot(q) * rhos[i]\n",
    "            q.add_(ys[i], alpha=-al[i])\n",
    "\n",
    "        # multiply by initial Hessian\n",
    "        # r/d is the final direction\n",
    "        d = r = torch.mul(q, H_diag)\n",
    "        for i in range(num_old):\n",
    "            be_i = ys[i].dot(r) * rhos[i]\n",
    "            r.add_(ss[i], alpha=al[i] - be_i)\n",
    "        \n",
    "        r = r.view(N,1)\n",
    "\n",
    "#         p = 1.0/s.view(-1).dot(y.view(-1))\n",
    "#         I = torch.eye(N)\n",
    "#         H = torch.matmul(torch.matmul(I-p*torch.matmul(s,y.t()), H), I-p*torch.matmul(y,s.t()))+p*torch.matmul(s,s.t())\n",
    "\n",
    "#         r = torch.matmul(H, w.grad)\n",
    "\n",
    "        if i == 1 or i % 1 == 0:\n",
    "            t_test_0 = time.time()\n",
    "            test_loss, test_accu = test_loss_accu(KX_test, Y_test, w, lambda_param)\n",
    "            t_test += time.time()-t_test_0\n",
    "\n",
    "            result = \"iteration {}: Time:{:.2f}, TrainLoss: {:.4f}, TrainAccu: {:.4f}, TestLoss: {:.4f}, TestAccu: {:.4f}\".format(idx, time.time()-t_test-t0,loss, accu, test_loss, test_accu)\n",
    "#             print(result)\n",
    "            filename = './results/LBFGS_{}_{}.txt'.format(learning_rate, lambda_param)\n",
    "            with open(filename, 'a') as fp: \n",
    "                fp.write(result+'\\n')\n",
    "\n",
    "        if w.grad.norm()<epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "history_size = 10\n",
    "learning_rates = [0.01]\n",
    "lambda_param = 1e-4\n",
    "epsilon = 1e-5\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    LBFGS(KX_train2, Y_train2, KX_test2, Y_test2, n_iterations,history_size,  learning_rate, lambda_param, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
